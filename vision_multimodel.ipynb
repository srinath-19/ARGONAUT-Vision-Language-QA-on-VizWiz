{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f828d61d-a2dc-496b-9f11-6baa964ba8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137a1e2f-3255-48c8-9377-3c1bc199f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Directory for all images\n",
    "img_dir = 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/'\n",
    "\n",
    "# Directory for annotation files\n",
    "ann_dir = 'https://vizwiz.cs.colorado.edu/VizWiz_final/vqa_data/Annotations/'\n",
    "\n",
    "train_annotation_path = '{}{}'.format(ann_dir, 'train.json')\n",
    "val_annotation_path = '{}{}'.format(ann_dir, 'val.json')\n",
    "test_annotation_path = '{}{}'.format(ann_dir, 'test.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58be4603-e752-4b8e-b5e4-5503b22b586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 20523\n",
      "Validation set size: 4319\n",
      "Test set size: 8000\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_data = requests.get(train_annotation_path, allow_redirects=True)\n",
    "train_data = train_data.json()\n",
    "\n",
    "# Validation\n",
    "val_data = requests.get(val_annotation_path, allow_redirects=True)\n",
    "val_data = val_data.json()\n",
    "\n",
    "# Test\n",
    "test_data = requests.get(test_annotation_path, allow_redirects=True)\n",
    "test_data = test_data.json()\n",
    "\n",
    "print('Train set size:', len(train_data))\n",
    "print('Validation set size:', len(val_data))\n",
    "print('Test set size:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347fdecf-8647-414f-8b72-72d81e9d2367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample: {'image': 'VizWiz_train_00001345.jpg', 'question': 'Hi can you tell me if this package of coffee is caffeinated or decaffeinated? Thank you.', 'answers': [{'answer_confidence': 'yes', 'answer': 'i dont know'}, {'answer_confidence': 'yes', 'answer': 'decaffeinated'}, {'answer_confidence': 'yes', 'answer': 'decaffeinated'}, {'answer_confidence': 'yes', 'answer': 'decaf'}, {'answer_confidence': 'yes', 'answer': 'decaf'}, {'answer_confidence': 'yes', 'answer': 'decaffeinated'}, {'answer_confidence': 'yes', 'answer': 'decaf'}, {'answer_confidence': 'maybe', 'answer': 'decaffeinated'}, {'answer_confidence': 'yes', 'answer': 'decaf'}, {'answer_confidence': 'yes', 'answer': 'decaffeinated'}], 'answer_type': 'other', 'answerable': 1}\n",
      "Second sample: {'image': 'VizWiz_val_00000000.jpg', 'question': 'Ok. There is another picture I hope it is a better one.', 'answers': [{'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'unanswerable', 'answer_confidence': 'maybe'}, {'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'unanswerable', 'answer_confidence': 'yes'}, {'answer': 'unanswerable', 'answer_confidence': 'no'}, {'answer': 'cannot repair this computer automatically', 'answer_confidence': 'maybe'}, {'answer': 'blank screen', 'answer_confidence': 'yes'}], 'answer_type': 'unanswerable', 'answerable': 0}\n",
      "Third sample: {'image': 'VizWiz_test_00000000.jpg', 'question': 'What is this? And what color is it?'}\n"
     ]
    }
   ],
   "source": [
    "vq = train_data[1345]\n",
    "a = val_data[0]\n",
    "b = test_data[0]\n",
    "\n",
    "# printing the entire set of annotation to see the structure\n",
    "print('First sample:', vq)\n",
    "print('Second sample:', a)\n",
    "print('Third sample:', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd8d484-3135-4d1f-bfe7-b4844e538b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_data[:1200] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab593935-07cd-439a-9ab8-e2c14ccc1fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': 'VizWiz_train_00001119.jpg', 'question': 'Hi can you tell me what this product is? ', 'answers': [{'answer_confidence': 'yes', 'answer': 'drink mix'}, {'answer_confidence': 'yes', 'answer': 'powdered green tea'}, {'answer_confidence': 'maybe', 'answer': 'unanswerable'}, {'answer_confidence': 'yes', 'answer': 'no'}, {'answer_confidence': 'yes', 'answer': 'no'}, {'answer_confidence': 'yes', 'answer': 'green tea'}, {'answer_confidence': 'maybe', 'answer': 'soup'}, {'answer_confidence': 'yes', 'answer': 'unanswerable'}, {'answer_confidence': 'no', 'answer': 'tea'}, {'answer_confidence': 'yes', 'answer': 'green tea'}], 'answer_type': 'other', 'answerable': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_subset[1119])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fcb4505-d48a-418d-88de-a686ed744399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: torch.Size([1200, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from concurrent.futures import ThreadPoolExecutor  # For parallel execution\n",
    "\n",
    "# Image transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Function to download & preprocess an image\n",
    "def process_image(sample):\n",
    "    image_url = img_dir + sample['image']\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=5)  # 5-second timeout\n",
    "        response.raise_for_status()  # Raise error if failed\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")  # Ensure 3 channels\n",
    "        return transform(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {image_url}: {e}\")\n",
    "        return torch.zeros(3, 224, 224)  # Return a blank image instead of crashing\n",
    "\n",
    "# Load images in parallel using ThreadPoolExecutor\n",
    "num_workers = 8  # Adjust based on system performance\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    image_tensors = list(executor.map(process_image, train_subset))\n",
    "\n",
    "# Stack tensors into a batch\n",
    "image_tensors = torch.stack(image_tensors)\n",
    "print('Train images shape:', image_tensors.shape)  # Expected: [num_samples, 3, 224, 224]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c22e94f-eb78-44a7-b85b-f176e92b92ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 633\n",
      "Train questions shape: torch.Size([1200, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer from spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenize and preprocess questions\n",
    "def tokenize_question(question):\n",
    "    doc = nlp(question.lower())  # Convert to lowercase\n",
    "    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Tokenize all questions\n",
    "tokenized_questions = [tokenize_question(sample['question']) for sample in train_subset]\n",
    "\n",
    "# Create a word-to-index vocabulary\n",
    "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}  # Special tokens\n",
    "for question in tokenized_questions:\n",
    "    for token in question:\n",
    "        if token not in word2idx:\n",
    "            word2idx[token] = len(word2idx)\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(word2idx)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# Convert tokenized questions into index sequences\n",
    "max_length = 20  # Fixed length for padding\n",
    "question_tensors = []\n",
    "for question in tokenized_questions:\n",
    "    seq = [word2idx.get(token, word2idx[\"<UNK>\"]) for token in question]\n",
    "    seq += [word2idx[\"<PAD>\"]] * (max_length - len(seq))  # Pad if shorter\n",
    "    seq = seq[:max_length]  # Truncate if longer\n",
    "    question_tensors.append(torch.tensor(seq))\n",
    "\n",
    "# Stack into a tensor\n",
    "question_tensors = torch.stack(question_tensors)\n",
    "print(\"Train questions shape:\", question_tensors.shape)  # Expected: [num_samples, max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c35c7f-6617-4995-831f-65d9947e869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_text(question, max_length=20):\n",
    "    encoding = tokenizer(question, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "    return encoding[\"input_ids\"].squeeze(0), encoding[\"attention_mask\"].squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aa721c1-5d1c-473d-bbfe-f97d4caed8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define Image Transformations (ViT-compatible)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 (ViT standard input size)\n",
    "    transforms.ToTensor(),          # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize (ViT prefers -1 to 1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "994dd3d8-8521-40b9-9a25-107290247f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class VizWizViTBERTDataset(Dataset):\n",
    "    def __init__(self, data, transform, max_length=20, base_url=\"https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/\"):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        self.base_url = base_url  # Use online images\n",
    "\n",
    "    def __len__(self):  \n",
    "        return len(self.data)  #  This fixes the error\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        #  Load Image from Online URL\n",
    "        image_url = self.base_url + sample[\"image\"]  # Construct URL\n",
    "        try:\n",
    "            response = requests.get(image_url, timeout=5)\n",
    "            response.raise_for_status()  # Ensure it's a valid response\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\" Failed to load image: {image_url} ({e})\")\n",
    "            image = torch.zeros(3, 224, 224)  # Return a blank image on failure\n",
    "\n",
    "        #  Tokenize Text\n",
    "        input_ids, attention_mask = tokenize_text(sample[\"question\"], self.max_length)\n",
    "\n",
    "        #  Convert Label to Tensor\n",
    "        label = torch.tensor(sample[\"answerable\"], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,  #  Now a tensor\n",
    "            \"input_ids\": input_ids,  #  Tensor\n",
    "            \"attention_mask\": attention_mask,  #  Tensor\n",
    "            \"label\": label  #  Tensor (0 or 1)\n",
    "        }\n",
    "\n",
    "# Create Dataset\n",
    "train_dataset = VizWizViTBERTDataset(train_subset, transform=image_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fcbd45d-339f-47ed-8554-353a4eefad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "047de63b-5b7e-4d98-8b60-ddbbb70469d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[\"image\"] for item in batch])  #  Stack image tensors\n",
    "    input_ids = pad_sequence([item[\"input_ids\"] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"image\": images,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"label\": labels\n",
    "    }\n",
    "\n",
    "#  Create DataLoader with `collate_fn`\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f7035d0-8004-4b72-929d-baa5fbae9292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[ 0.7020,  0.4118, -0.0431,  ...,  0.8118,  0.7961,  0.8510],\n",
      "         [ 0.6471,  0.3255, -0.0745,  ...,  0.8118,  0.8353,  0.8118],\n",
      "         [ 0.5843,  0.2392, -0.1137,  ...,  0.7804,  0.8353,  0.8275],\n",
      "         ...,\n",
      "         [-0.2706, -0.2706, -0.2706,  ..., -0.0667, -0.0745, -0.0431],\n",
      "         [-0.2863, -0.2863, -0.3098,  ..., -0.0824, -0.0745, -0.0745],\n",
      "         [-0.3098, -0.2941, -0.3020,  ..., -0.0980, -0.0824, -0.0902]],\n",
      "\n",
      "        [[ 0.3961,  0.1529, -0.2863,  ...,  0.2863,  0.2627,  0.3098],\n",
      "         [ 0.3490,  0.0588, -0.3098,  ...,  0.3020,  0.3098,  0.2863],\n",
      "         [ 0.3098, -0.0196, -0.3412,  ...,  0.2941,  0.3412,  0.3098],\n",
      "         ...,\n",
      "         [-0.5922, -0.5765, -0.5608,  ..., -0.2235, -0.2392, -0.2000],\n",
      "         [-0.6000, -0.5922, -0.6000,  ..., -0.2471, -0.2314, -0.2314],\n",
      "         [-0.6157, -0.5922, -0.5922,  ..., -0.2549, -0.2314, -0.2314]],\n",
      "\n",
      "        [[ 0.0902, -0.1137, -0.5137,  ..., -0.3725, -0.3725, -0.2941],\n",
      "         [ 0.1294, -0.1686, -0.5294,  ..., -0.3333, -0.2941, -0.3020],\n",
      "         [ 0.0510, -0.2627, -0.5686,  ..., -0.3255, -0.2627, -0.2627],\n",
      "         ...,\n",
      "         [-0.6863, -0.7020, -0.7176,  ..., -0.4118, -0.3882, -0.2863],\n",
      "         [-0.6941, -0.7098, -0.7412,  ..., -0.4196, -0.3804, -0.3333],\n",
      "         [-0.7333, -0.7020, -0.6941,  ..., -0.4039, -0.3569, -0.3647]]]), 'input_ids': tensor([ 101, 2054, 1005, 1055, 1996, 2171, 1997, 2023, 4031, 1029,  102,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])  # Check the dictionary keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b52eb82-c120-4b01-8c50-f0c597c9c950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Image Shape: torch.Size([32, 3, 224, 224])\n",
      "Batch Input IDs Shape: torch.Size([32, 20])\n",
      "Batch Attention Mask Shape: torch.Size([32, 20])\n",
      "Batch Label Shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "#  Fetch a Batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "#  Check Shapes\n",
    "print(\"Batch Image Shape:\", batch[\"image\"].shape)  # Expected: [32, 3, 224, 224]\n",
    "print(\"Batch Input IDs Shape:\", batch[\"input_ids\"].shape)  # Expected: [32, max_length]\n",
    "print(\"Batch Attention Mask Shape:\", batch[\"attention_mask\"].shape)  # Expected: [32, max_length]\n",
    "print(\"Batch Label Shape:\", batch[\"label\"].shape)  # Expected: [32]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e4692ef-3271-43c5-827e-7dd2daea4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel\n",
    "import timm  # For Vision Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c24c330-c6ad-4695-aba4-f6247683ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel\n",
    "import timm  \n",
    "\n",
    "class CustomViTBERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model=\"bert-base-uncased\", vit_model=\"vit_base_patch16_224\", num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        #  Vision Transformer (ViT) - REMOVE default classifier\n",
    "        self.vit = timm.create_model(vit_model, pretrained=True, num_classes=0)  # Removes classifier\n",
    "        vit_output_size = self.vit.num_features  # Correct output size from ViT\n",
    "\n",
    "        #  Reduce ViT Output Dimensionality (to match BERT)\n",
    "        self.vit_reduction = nn.Linear(vit_output_size, 768)  # Now input size is correct!\n",
    "\n",
    "        #  BERT Model - REMOVE default classifier\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        bert_output_size = self.bert.config.hidden_size  # 768\n",
    "\n",
    "        #  Attention-Based Fusion (Custom Design)\n",
    "        self.attention_layer = nn.MultiheadAttention(embed_dim=768, num_heads=8)\n",
    "\n",
    "        #  Custom Feature Fusion & Processing\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "\n",
    "        #  Custom Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "            nn.Softmax(dim=1)  # Output probabilities\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        #  Extract Image Features (ViT)\n",
    "        img_features = self.vit(image)  # Now this is [batch, 768]\n",
    "        img_features = self.vit_reduction(img_features)  # Reduce to 768 dims\n",
    "\n",
    "        #  Extract Text Features (BERT)\n",
    "        text_features = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "\n",
    "        #  Apply Custom Attention Mechanism\n",
    "        combined_features = torch.stack((img_features, text_features), dim=0)  # Stack instead of cat\n",
    "        combined_features, _ = self.attention_layer(combined_features, combined_features, combined_features)\n",
    "        combined_features = combined_features.mean(dim=0)  \n",
    "\n",
    "        #  Custom Feature Fusion\n",
    "        fused_features = self.fusion(combined_features)\n",
    "\n",
    "        #  Classifier Output\n",
    "        output = self.classifier(fused_features)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0ac0904-908d-42b8-9bf8-5b05de83cd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#  Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "#  Initialize Model & Move to GPU\n",
    "model = CustomViTBERTClassifier().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a53fcdae-5e58-453e-a723-be1d6abedb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)  # Fine-tuning with low LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5d96b823-7dab-4d96-90a2-9ed704457683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "84075e9f-266b-4f25-8a45-cb49e5183949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07f30847-034d-49d9-add6-c0092f1a687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 25.7323, Accuracy_cls: 0.5825\n",
      "TP: 476, TN: 223, FP: 137, FN: 364\n",
      "\n",
      "Epoch [2/5], Loss: 22.9753, Accuracy_cls: 0.7408\n",
      "TP: 646, TN: 243, FP: 117, FN: 194\n",
      "\n",
      "Epoch [3/5], Loss: 19.4737, Accuracy_cls: 0.8633\n",
      "TP: 752, TN: 284, FP: 76, FN: 88\n",
      "\n",
      "Epoch [4/5], Loss: 16.2384, Accuracy_cls: 0.9583\n",
      "TP: 821, TN: 329, FP: 31, FN: 19\n",
      "\n",
      "Epoch [5/5], Loss: 14.3011, Accuracy_cls: 0.9858\n",
      "TP: 832, TN: 351, FP: 9, FN: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        TP, TN, FP, FN = 0, 0, 0, 0  # True Positives, True Negatives, False Positives, False Negatives\n",
    "\n",
    "        for batch in train_loader:\n",
    "            #  Move Data to GPU\n",
    "            images = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, input_ids, attention_mask)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model weights\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            #  Predictions\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class (0 or 1)\n",
    "\n",
    "            #  Compute TP, TN, FP, FN\n",
    "            TP += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "            TN += ((predicted == 0) & (labels == 0)).sum().item()\n",
    "            FP += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "            FN += ((predicted == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "        #  Compute Accuracy using Correct Formula\n",
    "        accuracy_cls = (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "        #  Print Metrics for the Epoch (NO Debug Outputs)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy_cls: {accuracy_cls:.4f}\")\n",
    "        print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\\n\")\n",
    "\n",
    "#  Train the Model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8ae9bb8-53d9-4d10-8254-7b1d5f8179b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn1(batch):\n",
    "    images = torch.stack([item[\"image\"] for item in batch])  \n",
    "    input_ids = pad_sequence([item[\"input_ids\"] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch], batch_first=True, padding_value=0)\n",
    "\n",
    "    # ðŸ”¹ Only include labels if they exist in the batch (for training)\n",
    "    if \"label\" in batch[0]:  \n",
    "        labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
    "        return {\n",
    "            \"image\": images,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": labels\n",
    "        }\n",
    "    else:  \n",
    "        return {  \n",
    "            \"image\": images,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74930a60-d8c3-4ee3-bd35-8d2251842822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizWizTestDataset(Dataset):\n",
    "    def __init__(self, data, transform, max_length=20, base_url=\"https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/\"):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        self.base_url = base_url  # Online images\n",
    "\n",
    "    def __len__(self):  \n",
    "        return len(self.data)  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        #  Load Image from Online URL\n",
    "        image_url = self.base_url + sample[\"image\"]\n",
    "        try:\n",
    "            response = requests.get(image_url, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\" Failed to load image: {image_url} ({e})\")\n",
    "            image = torch.zeros(3, 224, 224)  # Return a blank image on failure\n",
    "\n",
    "        #  Tokenize Text\n",
    "        input_ids, attention_mask = tokenize_text(sample[\"question\"], self.max_length)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,  \n",
    "            \"input_ids\": input_ids,  \n",
    "            \"attention_mask\": attention_mask  \n",
    "        }\n",
    "\n",
    "#  Create Test Dataset\n",
    "test_dataset = VizWizTestDataset(test_data[:100], transform=image_transform)  # First 100 samples only\n",
    "\n",
    "#  Create Test DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=collate_fn1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95dddd27-0ab7-45c1-a324-baf4746a50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_100_predictions(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class (0 or 1)\n",
    "\n",
    "            predictions.extend(predicted.cpu().tolist())\n",
    "\n",
    "            if len(predictions) >= 100:  \n",
    "                break  # Stop after first 100 samples\n",
    "\n",
    "    return predictions[:100]  \n",
    "\n",
    "#  Get First 100 Predictions\n",
    "first_100_predictions = get_first_100_predictions(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b3ba08b-d79f-4520-b5e2-531f2ecaf02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Predictions saved successfully to Srinath_Muppala_challenge1.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "predictions_tensor = torch.tensor(first_100_predictions)  # 1D tensor\n",
    "\n",
    "#  Save the tensor to a .pkl file\n",
    "file_name = \"Srinath_Muppala_challenge1.pkl\"  # Change this to your actual name\n",
    "torch.save(predictions_tensor, file_name)\n",
    "\n",
    "print(f\" Predictions saved successfully to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7312db1-3654-4416-a13d-90fa662aa253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Shape: torch.Size([100])\n",
      "First 10 Predictions: [1, 0, 1, 1, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the predictions from the file\n",
    "predictions = torch.load(\"Srinath_Muppala_challenge1.pkl\")\n",
    "\n",
    "# Print the shape and first few values\n",
    "print(\"Predictions Shape:\", predictions.shape)  # Expected: torch.Size([100])\n",
    "print(\"First 10 Predictions:\", predictions[:10].tolist())  # Convert to list for easy reading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "187a1363-422e-4805-ab93-332bfeb9a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image': 'VizWiz_test_00000000.jpg', 'question': 'What is this? And what color is it?'}, {'image': 'VizWiz_test_00000001.jpg', 'question': 'What is this?'}, {'image': 'VizWiz_test_00000002.jpg', 'question': 'Has this oven gotten up to four hundred fifty degrees Fahrenheit yet?'}, {'image': 'VizWiz_test_00000003.jpg', 'question': 'What is this?'}, {'image': 'VizWiz_test_00000004.jpg', 'question': 'What is this?'}, {'image': 'VizWiz_test_00000005.jpg', 'question': 'What kind of key is this?'}, {'image': 'VizWiz_test_00000006.jpg', 'question': 'What does it say on here?'}, {'image': 'VizWiz_test_00000007.jpg', 'question': 'What is this? '}, {'image': 'VizWiz_test_00000008.jpg', 'question': 'What is this? What is this? '}, {'image': 'VizWiz_test_00000009.jpg', 'question': 'Do these beans look like black beans or pinto beans?'}]\n"
     ]
    }
   ],
   "source": [
    "print(test_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "656c2e9e-6b5d-4ff6-9ec6-62adb0114f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from transformers import BertModel\n",
    "\n",
    "class CustomViTBERTClassifier1(nn.Module):\n",
    "    def __init__(self, bert_model=\"bert-base-uncased\", vit_model=\"vit_base_patch16_224\", hidden_size=512, vocab_size=30522, top_k=5):\n",
    "        super(CustomViTBERTClassifier1, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # ViT Encoder\n",
    "        self.vit = timm.create_model(vit_model, pretrained=True)\n",
    "        self.vit.head = nn.Identity()\n",
    "        self.vit_output_size = self.vit.num_features\n",
    "\n",
    "        # BERT Encoder\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.bert_output_size = self.bert.config.hidden_size\n",
    "\n",
    "        # Project to hidden size\n",
    "        self.projection = nn.Linear(self.vit_output_size + self.bert_output_size, hidden_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Embedding for tokens\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        # Final vocab prediction\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask, answer_tokens=None):\n",
    "        img_features = self.vit(image)\n",
    "        text_features = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        combined = torch.cat((img_features, text_features), dim=1)\n",
    "        projected = self.projection(combined)\n",
    "\n",
    "        if answer_tokens is not None:\n",
    "            # Training Mode\n",
    "            decoder_input = self.embedding(answer_tokens)\n",
    "            h0 = projected.unsqueeze(0).repeat(self.decoder.num_layers, 1, 1)\n",
    "            c0 = torch.zeros_like(h0)\n",
    "            decoder_outputs, _ = self.decoder(decoder_input, (h0, c0))\n",
    "            output_logits = self.fc(decoder_outputs)\n",
    "            return output_logits\n",
    "        else:\n",
    "            # Inference Mode with Top-k Sampling\n",
    "            batch_size = image.size(0)\n",
    "            current_token = torch.full((batch_size, 1), 101, dtype=torch.long, device=image.device)\n",
    "            current_input = self.embedding(current_token)\n",
    "            h0 = projected.unsqueeze(0).repeat(self.decoder.num_layers, 1, 1)\n",
    "            c0 = torch.zeros_like(h0)\n",
    "            hidden = (h0, c0)\n",
    "\n",
    "            generated_tokens = []\n",
    "\n",
    "            for _ in range(20):\n",
    "                output, hidden = self.decoder(current_input, hidden)\n",
    "                logits = self.fc(output[:, -1, :])  # [B, vocab_size]\n",
    "\n",
    "                if self.top_k > 0:\n",
    "                    topk_vals, topk_idx = torch.topk(logits, self.top_k, dim=-1)\n",
    "                    probs = torch.nn.functional.softmax(topk_vals, dim=-1)\n",
    "                    next_token = topk_idx.gather(-1, torch.multinomial(probs, num_samples=1))\n",
    "                else:\n",
    "                    next_token = logits.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "                generated_tokens.append(next_token)\n",
    "                current_input = self.embedding(next_token)\n",
    "\n",
    "            return torch.cat(generated_tokens, dim=1)  # [B, 20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9aed160e-06de-44b5-a3d9-4a9d631740b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn_textgen(batch):\n",
    "    images = torch.stack([item[\"image\"] for item in batch])  #  Stack image tensors\n",
    "    input_ids = pad_sequence([item[\"input_ids\"] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch], batch_first=True, padding_value=0)\n",
    "\n",
    "    #  Fix: Use answer_tokens instead of label\n",
    "    answer_tokens = pad_sequence([item[\"answer_tokens\"] for item in batch], batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        \"image\": images,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"answer_tokens\": answer_tokens  #  Use this instead of \"label\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4ebee606-0c33-43be-a389-36106ce5f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizWizTextGenDataset(Dataset):\n",
    "    def __init__(self, data, transform, tokenizer, max_length=20, base_url=\"https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/\"):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        #  Load Image from Online URL\n",
    "        image_url = self.base_url + sample[\"image\"]\n",
    "        try:\n",
    "            response = requests.get(image_url, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\" Failed to load image: {image_url} ({e})\")\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "\n",
    "        #  Tokenize Question\n",
    "        input_ids, attention_mask = tokenize_text(sample[\"question\"], self.max_length)\n",
    "\n",
    "        #  Tokenize Answer (if available)\n",
    "        if \"answers\" in sample:\n",
    "            answer_text = sample[\"answers\"][0][\"answer\"]\n",
    "            answer_tokens = self.tokenizer(answer_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "        else:\n",
    "            answer_tokens = torch.zeros(self.max_length, dtype=torch.long)  # Placeholder\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"answer_tokens\": answer_tokens\n",
    "        }\n",
    "\n",
    "#  Initialize Dataset\n",
    "train_dataset = VizWizTextGenDataset(train_subset, transform=image_transform, tokenizer=tokenizer)\n",
    "\n",
    "#  DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn_textgen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b667f998-c161-4192-8f90-268e45c2d3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[ 0.7020,  0.4118, -0.0431,  ...,  0.8118,  0.7961,  0.8510],\n",
      "         [ 0.6471,  0.3255, -0.0745,  ...,  0.8118,  0.8353,  0.8118],\n",
      "         [ 0.5843,  0.2392, -0.1137,  ...,  0.7804,  0.8353,  0.8275],\n",
      "         ...,\n",
      "         [-0.2706, -0.2706, -0.2706,  ..., -0.0667, -0.0745, -0.0431],\n",
      "         [-0.2863, -0.2863, -0.3098,  ..., -0.0824, -0.0745, -0.0745],\n",
      "         [-0.3098, -0.2941, -0.3020,  ..., -0.0980, -0.0824, -0.0902]],\n",
      "\n",
      "        [[ 0.3961,  0.1529, -0.2863,  ...,  0.2863,  0.2627,  0.3098],\n",
      "         [ 0.3490,  0.0588, -0.3098,  ...,  0.3020,  0.3098,  0.2863],\n",
      "         [ 0.3098, -0.0196, -0.3412,  ...,  0.2941,  0.3412,  0.3098],\n",
      "         ...,\n",
      "         [-0.5922, -0.5765, -0.5608,  ..., -0.2235, -0.2392, -0.2000],\n",
      "         [-0.6000, -0.5922, -0.6000,  ..., -0.2471, -0.2314, -0.2314],\n",
      "         [-0.6157, -0.5922, -0.5922,  ..., -0.2549, -0.2314, -0.2314]],\n",
      "\n",
      "        [[ 0.0902, -0.1137, -0.5137,  ..., -0.3725, -0.3725, -0.2941],\n",
      "         [ 0.1294, -0.1686, -0.5294,  ..., -0.3333, -0.2941, -0.3020],\n",
      "         [ 0.0510, -0.2627, -0.5686,  ..., -0.3255, -0.2627, -0.2627],\n",
      "         ...,\n",
      "         [-0.6863, -0.7020, -0.7176,  ..., -0.4118, -0.3882, -0.2863],\n",
      "         [-0.6941, -0.7098, -0.7412,  ..., -0.4196, -0.3804, -0.3333],\n",
      "         [-0.7333, -0.7020, -0.6941,  ..., -0.4039, -0.3569, -0.3647]]]), 'input_ids': tensor([ 101, 2054, 1005, 1055, 1996, 2171, 1997, 2023, 4031, 1029,  102,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'answer_tokens': tensor([  101, 14732,  3727,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c512b28d-87c4-44c1-a9d0-b8b57f28d0b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[227], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#  Train the Model\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m train_textgen(model, train_loader, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[227], line 32\u001b[0m, in \u001b[0;36mtrain_textgen\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#  Compute loss only for non-padding tokens\u001b[39;00m\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), answer_tokens\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertModel\n",
    "import timm\n",
    "\n",
    "#  Define Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "model = CustomViTBERTClassifier1().to(device)\n",
    "\n",
    "def train_textgen(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            answer_tokens = batch[\"answer_tokens\"].to(device)  # Target sequence\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, input_ids, attention_mask, answer_tokens)\n",
    "\n",
    "            #  Compute loss only for non-padding tokens\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), answer_tokens.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            #  Compute Token Accuracy\n",
    "            predictions = outputs.argmax(dim=-1)  # Get most probable token for each position\n",
    "            mask = (answer_tokens != 0)  # Ignore padding tokens\n",
    "            correct = (predictions == answer_tokens) & mask\n",
    "            total_correct += correct.sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "#  Train the Model\n",
    "train_textgen(model, train_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952805a-da7c-450f-9067-742580c1a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizWizTextGenDataset(Dataset):\n",
    "    def __init__(self, data, transform, tokenizer, max_length=20, base_url=\"https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/\"):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        #  Load Image from URL\n",
    "        image_url = self.base_url + sample[\"image\"]\n",
    "        try:\n",
    "            response = requests.get(image_url, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\" Failed to load image: {image_url} ({e})\")\n",
    "            image = torch.zeros(3, 224, 224)  # Placeholder if image fails to load\n",
    "\n",
    "        #  Tokenize Question\n",
    "        input_ids, attention_mask = tokenize_text(sample[\"question\"], self.max_length)\n",
    "\n",
    "        #  For training, extract answers (not applicable to test set)\n",
    "        if \"answers\" in sample:  \n",
    "            answer_text = sample[\"answers\"][0] if sample[\"answers\"] else \"\"\n",
    "            answer_tokens = self.tokenizer(answer_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "        else:\n",
    "            answer_tokens = None  # No ground-truth answer for test set\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"answer_tokens\": answer_tokens  # Will be `None` for test set\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23bc5f-f2af-49ab-b2ca-12e6bc4e77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn_textgen2(batch):\n",
    "    images = torch.stack([item[\"image\"] for item in batch])\n",
    "    input_ids = pad_sequence([item[\"input_ids\"] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch], batch_first=True, padding_value=0)\n",
    "\n",
    "    #  Check if 'answer_tokens' exists in the batch (for training data)\n",
    "    if \"answer_tokens\" in batch[0]:  \n",
    "        answer_tokens = pad_sequence(\n",
    "            [item[\"answer_tokens\"] for item in batch if item[\"answer_tokens\"] is not None], \n",
    "            batch_first=True, \n",
    "            padding_value=0\n",
    "        )\n",
    "    else:\n",
    "        answer_tokens = None  # Test set does not have answer_tokens\n",
    "\n",
    "    return {\n",
    "        \"image\": images,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"answer_tokens\": answer_tokens  # This will be `None` for test set\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85540b-c631-4fbc-a286-33694d3d45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn_textgen2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5488f7-a5eb-4c94-896d-8a860987e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(generated_ids, tokenizer):\n",
    "    answers = []\n",
    "    for ids in generated_ids:\n",
    "        text = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        cleaned = \" \".join([w for w in text.split() if not w.startswith(\"[unused\")])\n",
    "        answers.append(cleaned.strip())\n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd37e15-e204-42da-8a25-ddbd5402c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_answers(model, test_loader, device, tokenizer):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # ðŸ”¸ Generate token IDs\n",
    "            generated_ids = model(images, input_ids, attention_mask)\n",
    "\n",
    "            # ðŸ”¸ Clean decoded answers\n",
    "            answers = decode_predictions(generated_ids, tokenizer)\n",
    "\n",
    "            # ðŸ”¸ Build prediction JSON\n",
    "            for i in range(len(answers)):\n",
    "                predictions.append({\n",
    "                    \"image\": test_data[batch_idx * len(answers) + i][\"image\"],\n",
    "                    \"answer\": answers[i]\n",
    "                })\n",
    "\n",
    "    return predictions[:100]\n",
    "\n",
    "#  Generate Predictions\n",
    "predictions = generate_answers(model, test_loader, device, tokenizer)\n",
    "\n",
    "#  Save JSON File\n",
    "with open(\"Srinath_Muppala_challenge2.json\", \"w\") as f:\n",
    "    json.dump(predictions, f, indent=4)\n",
    "\n",
    "print(\" Predictions saved to Srinath_Muppala_challenge2.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "83d9e6a8-0296-4968-9a70-66df55375a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': 'VizWiz_test_00000000.jpg', 'question': 'What is this? And what color is it?'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ab39b1a5-f606-408c-ba7b-77b5e08acf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"image\": \"VizWiz_test_00000000.jpg\",\n",
      "        \"answer\": \"##ci devote aesthetictos regionalrable working granny knyed cosmetics duane adherents rang quickly drug metaphor auxiliary clinical 70th\"\n",
      "    },\n",
      "    {\n",
      "        \"image\": \"VizWiz_test_00000001.jpg\",\n",
      "        \"answer\": \"grams doorbell printed bahamas fated birthday \\u66f2 featured uk transitionvite revealing harvest doneitarian done \\u66f2 masonry pottery\\u2192\"\n",
      "    },\n",
      "    {\n",
      "        \"image\": \"VizWiz_test_00000002.jpg\",\n",
      "        \"answer\": \"##rs shrinkingask phosphate butte br august discusrable fragmentation costs candle navigate gael discus hbo kits 1899rable\"\n",
      "    },\n",
      "    {\n",
      "        \"image\": \"VizWiz_test_00000003.jpg\",\n",
      "        \"answer\": \"##dina hiv walled hiv facebook ydticaidae seo upwards activation activation rein imply 70thamericana visainationssay astros\"\n",
      "    },\n",
      "    {\n",
      "        \"image\": \"VizWiz_test_00000004.jpg\",\n",
      "        \"answer\": \"telugu kung loads history officials \\u66f2 surfing\\u2192 surfing final 70th are grit \\u0561 draftinggis \\u0561 devote \\u66f2\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#  Load JSON File\n",
    "with open(\"Srinath_Muppala_challenge2.json\", \"r\") as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "#  Print First 10 Predictions\n",
    "print(json.dumps(predictions[:5], indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d8da645b-8335-4c76-9a06-5708ce6361b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[unused0]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([1]))  # Should print ['[unused0]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8de23a-ef2f-427f-9c94-2438dfca41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29384016-fd33-47c4-b9d4-c49d4a029dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7e4be7c6-112d-4233-862a-57665d302383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] basil leaves [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset[0]['answer_tokens']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "99d74d3b-286c-4ef0-b785-61e5fd7f2d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: fourteenth nipples restrictions springsteen surprising Êƒ Êƒ Êƒ Êƒ Êƒ Êƒ Êƒ Êƒ Êƒ Êƒ Êƒ Êƒ Êƒ Êƒ Êƒ\n",
      "Target: [CLS] basil leaves [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "sample = {k: v.unsqueeze(0).to(device) for k, v in sample.items()}\n",
    "output = model(**sample)\n",
    "predicted = output.argmax(dim=-1)\n",
    "print(\"Generated:\", tokenizer.decode(predicted[0]))\n",
    "print(\"Target:\", tokenizer.decode(sample['answer_tokens'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b39de-2a9c-41b6-8b54-29e8c44a4a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
